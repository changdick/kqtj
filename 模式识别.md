# 模式识别复习
## 模式识别基本概念
什么是模式  
什么是识别  
模式识别的含义  
模式识别和机器学习、神经网络、深度学习的关系  
模式识别系统包括的几个步骤   数据采集和预处理  特征生成  特征选择和提取（第五章讲） 识别分类（其他讲的算法都是）  
特征工程的含义  是什么决定了模式识别的上限
数据样本集的划分  （训练集、测试集， 或者划分成训练集、验证集、测试机）  
模式识别方法的分类    （有两种分类标准，按照数据样本是否已知类别标签，分为有监督学习和无监督学习；按照是否有一个明确的判别函数，鉴别式模型和生成式模型。贝叶斯属于生成式，聚类属于无监督，线性、非线性、距离分类器属于有监督中的鉴别式  
## 距离分类器
将待识别样本x 分类到与其**最相似（即：特征空间内距离最接近）**。这个距离就是一种类别可分性判据。  
模板匹配：每个类选出一个“模板”，模板就是某个类的代表样本。用代表样本来代替某个类，待分类样本x，和每个类的代表样本计算距离，取最近的那个类。  

最近邻分类：待分类样本和其他训练样本求距离，以距离最近的那个样本的类作为目标样本的类别。  特点：对噪声敏感，一个噪声会导致一大片错误。

k-近邻分类：待分类样本和其他训练样本求距离，然后取出距离最近的k个，这k个进行一个投票，在这k个中样本最多的类别就是待分类样本的类别。   特点：不平衡数据会不公平，因为样本多的类总占优势。  

## 分类器的评价
分类器评价的性能  （两个）  
什么样的分类器是个好的分类器  
完美区分训练数据的，是不是一个好的分类器？   
过拟合和欠拟合  
评价分类器的评价指标：准确率，错误率，拒识率。 敏感性、特异性  ROC曲线 （面积越大越好）   
> 准确率：分类正确的比率。只关注分类的结果预测对了，不关注是正面的对还是反面的对。
> 只看准确率，不能够反映出模型好坏，因为数据可能不平衡，所以单看准确率看不出模型预测的准不准，其次，错误是有代价的，准确率反应不出犯错的代价；还要看针对正面样本命中的概率，负面样本命中的概率。这就是敏感性、特异性。  
> 准确率不适用于不平衡数据集，准确率无法正确反映分类错误的代价，因为并非所有错误的代价都相等。  
> 敏感性：正面样本预测中的比率。 特异性：负面样本预测中的比率。 用测疾病的阴阳性的场景，敏感性=真阳/（假阴+真阳），特异性=真阴/（假阳+真阴）   

评价方法：交叉验证

## 统计分类器及其学习
生成式模型的理论基础是贝叶斯决策模型。
### 贝叶斯决策过程
想想已知是什么（先验概率和类条件概率密度函数），求出来的依据是什么（后验概率），根据后验概率，如何确定最后判给哪一个类（最小错误率，最小风险两种方法）
### 类条件概率密度函数的求解：参数估计和非参数估计两种。
### 参数估计中的最大似然估计
假如参数是 $\theta$,拿到了样本 $x$,既然样本 $x$发生了，那么参数 $\theta$是最有可能使得样本为 $x$的。直接把 $P(x|\theta)$取最大值的时候，这个式子是几个概率相乘得到，取对数，求导，即可求出极值点。就估计到了 $\theta$的值。  
除了最大似然估计，还有贝叶斯估计。  
### 概率密度函数的非参数估计
核心思想：把空间划分成小区域。假如有n个样本，某个区域体积为V，在这个区域里找到k个样本，那么样本在这个区域的概率是 $frac{k}{n}$。这个区域里面视为均匀分布，所以这个区域里的概率密度就是 $\frac{k}{nV}$.  
但是小区域划分多少块，每个区域V多大，走向两种具体方法：Parzen窗法，K-近邻法。  
**Parzen窗法**：区域体积V 是样本数n 的函数。 $V=\frac{1}{\sqrt{n}}$  以高斯窗为例，概率密度函数是一系列以样本点为中心的窗口函数之和   

**k-近邻法**：区域包含样本数k 是样本数n 的函数 $k=\sqrt{n}$    
k-近邻法直接估计后验概率

 ## 特征选择和提取
 两种降维方法  
 特征选择    （从原特征挑选一个子集）  
 特征提取     （从原特征通过函数变换生成新特征）   
 类别可分性判据  （两种）

 ### 主成分分析PCA
 求所有样本各个维度的协方差矩阵，协方差矩阵一定是实对称矩阵。因此有线性代数得到几个性质：有n个特征值，n特征向量且它们都正交。  
 求协方差矩阵的特征值和特征向量，特征向量就是一组新的坐标系，对应特征值表示这个方向上信息量大小。根据特征值，挑特征值大的方向保留下来，特征值几乎为0的舍弃。  
一般舍弃不超过5%的特征值。舍弃的越多，误差越多。  
PCA变换后特征不相关。  

冗余特征怎么看？  
特征向量的性质  
特征是否相关  

属于无监督学习，所有样本一起处理。  

### 基于fisher准则的可分性分析 fda 线性判别分析 lda
坐标系不是正交的。变换后的特征是有相关性的。  
会根据样本的类别来变换，FDA的投影能够以最大可分性区别不同类数据。  
fisher准则量：分子是各类均值之差来的，表征了类间分散程度；分子是类内方差之和，表征了类内部集中程度；这个值越大说明越能区分样本。  
两个矩阵： $S_w,S_b$,分别对应分母和分子。求出一个矩阵 $S_w^{-1}S_b$，这个矩阵再求特征值和特征向量，特征值表示对应特征向量方向投影的可区分程度。  

 
